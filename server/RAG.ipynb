{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f947b1b",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Chat:\n",
    "from operator import itemgetter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# History\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain_core.runnables import RunnableWithMessageHistory, RunnablePassthrough\n",
    "# Load\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# Store\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# Retrieve\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "# from llm import get_response_stream, get_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9034491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in get_response(\"hello\", dummy=True):\n",
    "#     print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e0efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9fd40ed",
   "metadata": {},
   "source": [
    "## LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d88e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 16000\n",
    "PER_DOC_TOKENS = 750\n",
    "SYS_PROMPT_SIZE = 1000 # assumed\n",
    "TOTAL_DOC_SIZE = 3000\n",
    "DOC_COUNT = TOTAL_DOC_SIZE // PER_DOC_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28166b6",
   "metadata": {},
   "source": [
    "### Ollama - Gemma3:4b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Gemma3 context size -> 128K (1,31,072)\n",
    "# 30k -> 91% RAM, 91% GPU\n",
    "# 25k -> 82% RAM, 89% GPU\n",
    "# 15k -> 66% RAM, 87% GPU\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:latest\", temperature=1,\n",
    "    # model=\"gemma3:1b\", temperature=1,\n",
    "    #  num_predict=MAX_OUTPUT_TOKENS,\n",
    "    num_gpu=35, num_ctx=MAX_TOKENS\n",
    ")\n",
    "llm.invoke(\"Hii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e656b3",
   "metadata": {},
   "source": [
    "### Groq - Llama3:70B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c84435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_groq import ChatGroq\n",
    "# llm = ChatGroq(\n",
    "#     model=\"llama-3.3-70b-versatile\", temperature=\"1\",\n",
    "#     max_tokens=MAX_TOKENS, api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "# )\n",
    "# llm.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e663687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown(llm.invoke(\"write a story\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa74252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef563fc",
   "metadata": {},
   "source": [
    "## Template:\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <strong>Limitations:</strong> Listed are some base assumptions in certain components of langchain components.\n",
    "</div>\n",
    "\n",
    "- `CreateHistoryAwareRetriever` assumes the latest-user-message key to be `input`\n",
    "- `Trimmer` assumes the `ChatHistory` key to be `messages`\n",
    "- `CreateStuffDocumentChain` assumes returns the clubbed `docs` in key `context`\n",
    "- To overcome this, you need to use `RunnablePassthrough` or RunnableMap and assign those keys and variables accordingly.\n",
    "- But remember, you need to manually set such things for all the variables which u are using different than default.\n",
    "\n",
    "- So it's always good to follow the default keys and avoid complexity in chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078666d",
   "metadata": {},
   "source": [
    "### Chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca015456",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_chat = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\",  \"\".join([\n",
    "            \"You are a highly knowledgeable and helpful AI assistant.\\n\"\n",
    "            \"You are provided with the user's chat history and external documents to assist in your response.\\n\\n\"\n",
    "            \"Your task is to:\\n\"\n",
    "            \"- Accurately and clearly answer the user's latest question.\\n\"\n",
    "            \"- Incorporate any relevant information from the context documents enclosed below.\\n\"\n",
    "            # \"- Reference the source(s) whenever applicable.\\n\"\n",
    "            \"- Use appropriate markdown formatting for clarity and readability (e.g., bullet points, headings, code blocks, tables).\\n\\n\"\n",
    "            \"- If not available in the context, mention that and then answer from your own knowledge.\\n\"\n",
    "            \"Contextual Documents:\\n\"\n",
    "            \"<CONTEXT>{context}</CONTEXT>\"\n",
    "        ])),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input} \\n\\n **Strictly stick to the instructions!**\")\n",
    "    ]\n",
    ")\n",
    "template_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tokens in this System message and pass rest of the max possible chat history:\n",
    "# trim_keep = model_context - template_tokens - 250 (safe side)\n",
    "# template_chat.messages[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05028a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6659d3b3",
   "metadata": {},
   "source": [
    "### Summarize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d263f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_summarize = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", \"\".join([\n",
    "            \"You are an expert at summarizing conversations into standalone prompts.\\n\"\n",
    "            \"You are given a complete chat history, ending with the user's latest message.\\n\\n\"\n",
    "            \"Your task is to:\\n\"\n",
    "            \"- Understand the entire conversation context.\\n\"\n",
    "            \"- Identify references in the latest user message that relate to earlier messages.\\n\"\n",
    "            \"- Create a single clear, concise, and standalone question or prompt.\\n\"\n",
    "            \"- This final prompt should be fully understandable without needing the prior conversation.\\n\"\n",
    "            \"- It will be used to retrieve the most relevant documents.\\n\\n\"\n",
    "            \"Only return the rewritten standalone prompt. Do not add explanations or formatting.\"\n",
    "        ])),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}. \\n\\n **Make one standalone prompt as asked!**\")\n",
    "    ]\n",
    ")\n",
    "template_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tokens in this System message and pass rest of the max possible chat history:\n",
    "# trim_keep = model_context - template_tokens - (1000tok/doc * n-docs) - 250 (safe side)\n",
    "# template_summarize.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fa2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e81f3ce2",
   "metadata": {},
   "source": [
    "## Chat Message History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    # print(\"*\"*40, session_id, \"*\"*40)\n",
    "    if session_id not in chat_histories:\n",
    "        chat_histories[session_id] = ChatMessageHistory()\n",
    "        # log here for creation of new chat history\n",
    "        print(f\"Created chat hist for session id: `{session_id}`\")    \n",
    "    return chat_histories[session_id]\n",
    "\n",
    "get_session_history(\"abv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8348a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_session_history(\"abv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d912cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e8716e2",
   "metadata": {},
   "source": [
    "### Trimmer:\n",
    "- Due to some un-known issue in variable name of messages being \"chat_history\" or \"messages\" the trimmer cant be used in this RAG implementation.\n",
    "- Reason: Trimmer expects \"messages\"\n",
    "- But, if i use \"messages\", then idk why, the summarizer step does not call LLM at all, it just does not work, and is completely untraceable.\n",
    "- Still, if u want to implement, use one runnable_passthrough before the trimmer in chain to convert chat_history > messages and the after its response, output > chat_history again! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39350a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For summary 15k chat + 1k system and all\n",
    "# trim_summary = trim_messages(\n",
    "#     max_tokens=MAX_TOKENS - SYS_PROMPT_SIZE,\n",
    "#     strategy=\"last\", token_counter=llm, start_on=\"human\",\n",
    "#     allow_partial=True,  # include_system=True,\n",
    "# )\n",
    "\n",
    "# # For chat 10k chat + 5*1k docs + 1k system and all\n",
    "# trim_chat = trim_messages(\n",
    "#     max_tokens=MAX_TOKENS - (TOTAL_DOC_SIZE) - SYS_PROMPT_SIZE,\n",
    "#     strategy=\"last\", token_counter=llm, start_on=\"human\",\n",
    "#     allow_partial=True,  # include_system=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b73aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a387c0",
   "metadata": {},
   "source": [
    "## VectorStore:\n",
    "### Embeddings:\n",
    "- Notice that the embeddings are not offloaded at all to the GPU\n",
    "- This is done because the Ollama repeatedly keeps loading and un-loading the emb / llm in each call.\n",
    "- Even when I have memory, IDK why ollama loads only one of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1fa76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\", num_gpu=0)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e69b0",
   "metadata": {},
   "source": [
    "### Loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = PyMuPDFLoader(file_path=\"./assets/pdf_w_text.pdf\", extract_tables='markdown', extract_images=True).load()\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927b8aa",
   "metadata": {},
   "source": [
    "### Splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5247c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=PER_DOC_TOKENS, chunk_overlap=150,\n",
    ")\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d840a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afd1b33c",
   "metadata": {},
   "source": [
    "### Database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c108c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = splitter.split_documents(file)\n",
    "splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initialization needs 4 param, so rather moving to adding one doc manually.\n",
    "database = FAISS.from_documents(documents=splitted, embedding=embeddings)\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dba060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(splitted[0].page_content))\n",
    "print(len(splitted[0].page_content.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b24c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9df58878",
   "metadata": {},
   "source": [
    "### Retriever:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfabaf",
   "metadata": {},
   "source": [
    "- So for 750 chars, there are appx 95 word (max 150)\n",
    "- In order to retrieve the 3k tokens, we need to have 3k/150 = 20 chunks\n",
    "- So, set k=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01408ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = database.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 20}\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"fun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b38c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323de82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a1aca4c",
   "metadata": {},
   "source": [
    "## Summarizer:\n",
    "\n",
    "- Old method.\n",
    "- This is too much hard-coded, switch to the retrieval method with the create_stuff_chain to ingest the documents and get the answer in one single chain call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = (\n",
    "#     RunnablePassthrough().assign(messages=itemgetter(\"messages\") | trim_chat)\n",
    "#     | template_summarize | llm | StrOutputParser())\n",
    "\n",
    "# summarizer_llm = RunnableWithMessageHistory(\n",
    "#     runnable=chain,\n",
    "#     get_session_history=get_session_history,\n",
    "#     input_messages_key=\"input\",\n",
    "#     history_messages_key=\"messages\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_histories[10] = ChatMessageHistory()\n",
    "# chat_histories[10].messages = [\n",
    "#     HumanMessage(\"Hello, I'm Bhushan, What is your name?\"),\n",
    "#     AIMessage(\"I am an AI assistant. I am not a human like you.\"),\n",
    "#     HumanMessage(\"What is Artificial General Intelligence?\"),\n",
    "#     AIMessage(\"Artificial General Intelligence (AGI) refers to highly autonomous systems that outperform humans at most economically valuable work.\"),\n",
    "# ]\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer_llm.invoke(\n",
    "#     input={\"input\": \"So it's not achieved yet?\", },\n",
    "#     config={\"configurable\": {\"session_id\": 10}}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_histories[10].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba9fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483077d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3fb841d",
   "metadata": {},
   "source": [
    "## Runnable With History:\n",
    "- Commented out as it's un-necessary and not used in the code.\n",
    "- But, keep it, as it can be used in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = (\n",
    "#     RunnablePassthrough(name=\"Trim Chat History\").assign(messages=itemgetter(\"messages\") | trim_chat)\n",
    "#     | template_chat | llm | StrOutputParser())\n",
    "\n",
    "# chat_llm = RunnableWithMessageHistory(\n",
    "#     runnable=chain,\n",
    "#     get_session_history=get_session_history,\n",
    "#     input_messages_key=\"input\",\n",
    "#     history_messages_key=\"messages\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff80224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_llm.invoke(\n",
    "#     input={\n",
    "#         \"input\": \"Hello, I'm Bhushan, What is your name?\",\n",
    "#         \"context\": \"This is some random document which contains some random information.\"\n",
    "#     },\n",
    "#     config={\n",
    "#         \"configurable\": {\n",
    "#             \"session_id\": 15\n",
    "#         }\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d49b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_llm.invoke(\n",
    "#     input={\n",
    "#         \"input\": \"What did we discuss?\",\n",
    "#         \"context\": \"There is no context available for this question.\"\n",
    "#     },\n",
    "#     config={\n",
    "#         \"configurable\": {\n",
    "#             \"session_id\": 15\n",
    "#         }\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572626cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a284c06c",
   "metadata": {},
   "source": [
    "- If () add option to paste link and scrap whole content from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1d7ad",
   "metadata": {},
   "source": [
    "## Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a passthrough which prints variables and passes them to next step\n",
    "# def print_and_pass(input):\n",
    "#     print(input)\n",
    "#     return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 User Input + Chat History > Summarizer Template > Standalone Que > Get Docs\n",
    "summarize_chain = create_history_aware_retriever(llm, retriever, template_summarize)\n",
    "# summarize_chain = trim_summary | create_history_aware_retriever(llm, retriever, template_summarize)\n",
    "\n",
    "# 4 Multiple Docs > Combine All > Chat Template > Final Output\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=template_chat)\n",
    "\n",
    "# 2 Input + Chat History > [ `Summarizer Template` > `Get Docs` ] > [ `Combine` > `Chat Template` ] > Output\n",
    "rag_chain = create_retrieval_chain(summarize_chain , qa_chain)\n",
    "\n",
    "# 1 Final main chain:\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    runnable=rag_chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "conversational_rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa3a7d",
   "metadata": {},
   "source": [
    "## Test:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e77ef7",
   "metadata": {},
   "source": [
    "### Database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.add_documents(\n",
    "    [\n",
    "        Document(\"Cats and Dogs are both popular pets.\"),\n",
    "        Document(\"Cats are independent and low-maintenance pets.\"),\n",
    "        Document(\"Dogs are loyal and require more attention.\"),\n",
    "        Document(\"Cats are often seen as aloof and mysterious.\"),\n",
    "        Document(\"Dogs are known for their loyalty and companionship.\"),\n",
    "        Document(\"Cats are great for small living spaces.\"),\n",
    "        Document(\"Cats are NOT AT ALL LOYAL.\"),\n",
    "    ],\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "database.search(search_type='similarity', query=\"animals\", k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05049e4e",
   "metadata": {},
   "source": [
    "### Summarize Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbce540",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain.invoke(\n",
    "    input={\n",
    "        \"input\": \"What animal was i talking about? Which one is most common apart from that animal?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(\"Hello, I'm Bhushan, What is your name?\"),\n",
    "            AIMessage(\"I am an AI assistant. I am not a human like you.\"),\n",
    "            HumanMessage(\"What are ur thoughts on DOGs?\"),\n",
    "            AIMessage(\"Dogs are loyal and require more attention.\"), \n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e264d7",
   "metadata": {},
   "source": [
    "### QA - Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain.invoke(\n",
    "    input={\n",
    "        \"input\": \"Full form of RAG?\",\n",
    "        \"context\": [Document(page_content=\"This is some random document which contains some random information.\")],\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(\"hi\"),\n",
    "            AIMessage(\"hello\"),\n",
    "            HumanMessage(\"What is RAG?\"),\n",
    "            AIMessage(\"RAG is a technique to combine retrieval and generation.\"),\n",
    "        ]\n",
    "    },\n",
    "    # config={\"configurable\": {\"session_id\": 15}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9378f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dac4910",
   "metadata": {},
   "source": [
    "### RAG Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\n",
    "    input={\n",
    "        \"input\": \"Full form of RAG?\",\n",
    "        \"context\": [Document(page_content=\"This is some random document which contains some random information.\")],\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(\"hi\"),\n",
    "            AIMessage(\"hello\"),\n",
    "            HumanMessage(\"What is RAG?\"),\n",
    "            AIMessage(\"RAG is a technique to combine retrieval and generation.\"),\n",
    "        ]\n",
    "    },\n",
    "    # config={\"configurable\": {\"session_id\": 15}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1c344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d18165",
   "metadata": {},
   "source": [
    "### Conv RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    input={\"input\":\"Hello, I am Bhushan. What abt u?\"},\n",
    "    config={\"configurable\":{\"session_id\":120}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    input={\"input\":\"What are popular pets?\"},\n",
    "    config={\"configurable\":{\"session_id\":120}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    input={\"input\":\"Describe CATS?\"},\n",
    "    config={\"configurable\":{\"session_id\":120}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d17980",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    input={\"input\":\"1. Are they LOYAL? 2. What do I mean my THEY?\"},\n",
    "    config={\"configurable\":{\"session_id\":120}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4614c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c40c001",
   "metadata": {},
   "source": [
    "## Important:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df51717",
   "metadata": {},
   "source": [
    "- Just figured this out\n",
    "- if u are having LLM call in the chain, but still it is not working, the possible reason is `ChatPromptTemplate`.\n",
    "- If history is empty, then Template is skipped\n",
    "- And maybe hence, all further calls as well!!\n",
    "- So, if LLM is not getting called, try passing some history manually\n",
    "\n",
    "> My issue\n",
    "- I was using \"messages\" for the chat history\n",
    "- Cause, trimmer expects \"messages\" key for input\n",
    "- But, somehow, prompt template was not able to use \"messages\" key even though it was set explicitly like that.\n",
    "- Once replaced with \"chat_history\", it worked.\n",
    "- Also, for output always use \"answer\" key (in create hist aware retriever in Conversational RAG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
