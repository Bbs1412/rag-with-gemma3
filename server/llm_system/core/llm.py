from time import sleep
from random import choice
from typing import Generator

from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.language_models.chat_models import BaseChatModel as T_LLM

from logger import get_logger
log = get_logger(name="core_llm")


def get_llm(model_name: str, context_size: int,
            temperature: float, verify_connection: bool = False) -> T_LLM:
    """Get the LLM model with the specified parameters.

    Args:
        model_name (str): The name of the LLM model to use.
        context_size (int): The maximum context size for the model.
        temperature (float): The temperature setting for the model.
        verify_connection (bool): Whether to verify the connection to the model.

    Returns:
        BaseChatModel: An instance of the LLM model configured with the specified parameters.
    """

    log.info(f"Initializing LLM(model={model_name}, ctx_size={context_size}, temp={temperature})")
    model = ChatOllama(model=model_name, num_ctx=context_size, temperature=temperature)

    if verify_connection:
        try:
            _ = model.invoke("ping")
            log.info(f"LLM model '{model_name}' initialized and connection verified.")

        except Exception as e:
            log.error(f"Failed to initialize LLM model '{model_name}': {e}")
            raise RuntimeError(f"Could not initialize LLM model '{model_name}'") from e
    else:
        log.warning(f"LLM model '{model_name}' initialized without connection verification.")

    return model


def get_output_parser():
    """Get the output parser for the LLM model.

    Returns:
        StrOutputParser: An instance of StrOutputParser to parse the model's output.
    """
    log.info("Initializing the output parser for LLM responses.")
    return StrOutputParser()


# ------------------------------------------------------------------------------
# Dummy responses of LLM
# ------------------------------------------------------------------------------

# Resp in increasing order of length:
dummy_responses = [
    "1> Hey there! ðŸ‘‹  \nI'm Gemma-3, a language model developed by the Gemma team at Google. I'm here to help you with questions and create interesting text. Iâ€™m still improving and learning each day. Letâ€™s explore and have some fun together! ðŸ¤–",

    "2> Hello ðŸ‘‹!  \nI'm Gemma-3 ðŸ˜Ž, a large language model created by the Gemma team at Google-Deepmind. Iâ€™m here to assist you with a wide range of tasks, from answering your questions to generating creative text formats. My goal is to provide helpful and informative responses. I'm still under development, and Iâ€™m learning new things every day! Iâ€™m excited to explore with you. Let's see what we can create! ðŸ¤–âœ¨",

    "3> Hello ðŸ‘‹!  \nI'm Gemma-3 ðŸ˜Ž, a powerful language model developed by the talented folks at Google-Deepmind. I'm here to help you out with a wide range of tasksâ€”whether itâ€™s answering complex questions, crafting detailed explanations, writing stories, poems, or even generating code snippets. I strive to be informative, creative, and engaging in every response I give.  \n\nI'm constantly learning, improving, and adapting to serve you better. Even though I'm still a work in progress, I'm pretty good at what I do! ðŸ˜„  \n\nFeel free to test my capabilitiesâ€”ask me anything, challenge me, or just chat. Letâ€™s collaborate, learn new things, and build something awesome together. Ready when you are! ðŸš€ðŸ¤–âœ¨"
]
log.info(f"Loaded {len(dummy_responses)} dummy responses for testing.")


def get_dummy_response() -> str:
    """Get a dummy response from the predefined list."""
    return choice(dummy_responses)


def get_dummy_response_stream(batch_tokens: int, token_rate: int = 45) -> Generator[str, None, None]:
    """Get a dummy response from the predefined list."""
    delay = batch_tokens / token_rate
    dummy_response = choice(dummy_responses)

    for i in range(0, len(dummy_response.split(" ")), batch_tokens):
        yield " ".join(dummy_response.split(" ")[i:i + batch_tokens])+" "
        sleep(delay)
