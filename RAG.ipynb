{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f947b1b",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat:\n",
    "from operator import itemgetter\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# History\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain_core.runnables import RunnableWithMessageHistory, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from llm import get_response_stream, get_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9034491",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in get_response(\"hello\", dummy=True):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e0efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9078666d",
   "metadata": {},
   "source": [
    "### Chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_chat = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            \"You are a helpful assistant. You answer the question asked based on the chat history and also the Documents attached in context. Answer factually and clearly. State the source in answer wherever possible. Use various markdown features in response. \\n<CONTEXT>\\n{context}\\n</CONTEXT>\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        HumanMessage(\"{input}\")\n",
    "    ]\n",
    ")\n",
    "template_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05028a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6659d3b3",
   "metadata": {},
   "source": [
    "### Summarize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_summarize = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            \"You are a Summarizing expert. You are given with a complete chat history and the latest user message in end of it. The latest message might have some content which refers to some part in history. You have to compile everything and return a single prompt, which will have a standalone question which can be completely understood without any chat history. So, give me a single prompt which will be helpful in retrieving the most relevant docs to latest message.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        HumanMessage(\"{input}\")\n",
    "    ]\n",
    ")\n",
    "template_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fa2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e81f3ce2",
   "metadata": {},
   "source": [
    "## Chat Message History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    # print(\"*\"*40, session_id, \"*\"*40)\n",
    "    if session_id not in chat_histories:\n",
    "        chat_histories[session_id] = ChatMessageHistory()\n",
    "        # log here for creation of new chat history\n",
    "        print(f\"Created chat hist for session id: `{session_id}`\")    \n",
    "    return chat_histories[session_id]\n",
    "\n",
    "get_session_history(\"abv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8348a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_session_history(\"abv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd40ed",
   "metadata": {},
   "source": [
    "## LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "# Gemma3 context size -> 128K (1,31,072)\n",
    "# 30k -> 91% RAM, 91% GPU\n",
    "# 25k -> 82% RAM, 89% GPU\n",
    "# 15k -> 66% RAM, 87% GPU\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:latest\", temperature=\"1\",\n",
    "    #  num_predict=MAX_OUTPUT_TOKENS,\n",
    "    num_gpu=35, num_ctx=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e663687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown(llm.invoke(\"write a story\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d912cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e8716e2",
   "metadata": {},
   "source": [
    "## Trimmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39350a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# For summary 15k chat + 1k system and all\n",
    "trim_summary = trim_messages(\n",
    "    max_tokens=15000,\n",
    "    strategy=\"last\", token_counter=llm, start_on=\"human\",\n",
    "    allow_partial=True,  # include_system=True,\n",
    ")\n",
    "\n",
    "# For chat 10k chat + 5*1k docs + 1k system and all\n",
    "trim_chat = trim_messages(\n",
    "    max_tokens=10000,\n",
    "    strategy=\"last\", token_counter=llm, start_on=\"human\",\n",
    "    allow_partial=True,  # include_system=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b73aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a1aca4c",
   "metadata": {},
   "source": [
    "## Summarizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough().assign(messages=itemgetter(\"messages\") | trim_chat)\n",
    "    | template_summarize | llm | StrOutputParser())\n",
    "\n",
    "summarizer_llm = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_histories[10] = ChatMessageHistory()\n",
    "chat_histories[10].messages = [\n",
    "    HumanMessage(\"Hello, I'm Bhushan, What is your name?\"),\n",
    "    AIMessage(\"I am an AI assistant. I am not a human like you.\"),\n",
    "    HumanMessage(\"What is Artificial General Intelligence?\"),\n",
    "    AIMessage(\"Artificial General Intelligence (AGI) refers to highly autonomous systems that outperform humans at most economically valuable work.\"),\n",
    "]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_llm.invoke(\n",
    "    input={\"input\": \"So it's not achieved yet?\", },\n",
    "    config={\"configurable\": {\"session_id\": 10}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_histories[10].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba9fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483077d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3fb841d",
   "metadata": {},
   "source": [
    "## Runnable With History:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough(name=\"Trim Chat History\").assign(messages=itemgetter(\"messages\") | trim_chat)\n",
    "    | template_chat | llm | StrOutputParser())\n",
    "\n",
    "chat_llm = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff80224",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm.invoke(\n",
    "    input={\n",
    "        \"input\": \"Hello, I'm Bhushan, What is your name?\",\n",
    "        \"context\": \"This is some random document which contains some random information.\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"session_id\": 15\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d49b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm.invoke(\n",
    "    input={\n",
    "        \"input\": \"What did we discuss?\",\n",
    "        \"context\": \"There is no context available for this question.\"\n",
    "    },\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"session_id\": 15\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572626cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a284c06c",
   "metadata": {},
   "source": [
    "- If () add option to paste link and scrap whole content from there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
